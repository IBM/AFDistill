diff --git a/training/training.py b/training/training.py
index dd8135b..ac00db8 100644
--- a/training/training.py
+++ b/training/training.py
@@ -1,3 +1,6 @@
+import sys
+sys.path.insert(0, '/path/to/AFDistill')
+from model.litdistiller import LitDistiller
 import argparse
 import os.path
 
@@ -106,6 +109,7 @@ def main(args):
         for i in range(3):
             q.put_nowait(executor.submit(get_pdbs, train_loader, 1, args.max_protein_length, args.num_examples_per_epoch))
             p.put_nowait(executor.submit(get_pdbs, valid_loader, 1, args.max_protein_length, args.num_examples_per_epoch))
+
         pdb_dict_train = q.get().result()
         pdb_dict_valid = p.get().result()
        
@@ -114,6 +118,8 @@ def main(args):
         
         loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)
         loader_valid = StructureLoader(dataset_valid, batch_size=args.batch_size)
+
+        afdistill = LitDistiller.load_from_checkpoint(checkpoint_path='/path/to/AFDistill/output/plddt_v1/checkpoints/checkpoint_last.ckpt', alphabet_downstream='ACDEFGHIKLMNPQRSTVWY').cuda()
         
         reload_c = 0 
         for e in range(args.num_epochs):
@@ -142,9 +148,17 @@ def main(args):
                 
                 if args.mixed_precision:
                     with torch.cuda.amp.autocast():
-                        log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
+                        log_probs, logits = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
                         _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)
-           
+
+                    AFlogits = []
+                    for lgt, m in zip(logits, mask_for_loss):
+                        AFlogits.append(lgt[m>0][:,:-1])  # mask out unwanted part, and take only 20 amino acids
+
+                    # apply structure consistency loss using AFDistill
+                    sc_loss = afdistill.sc_loss(AFlogits).mean()
+                    loss_av_smoothed += 1.0 * torch.exp(-sc_loss) # modify weighting (1.0), if necessar
+
                     scaler.scale(loss_av_smoothed).backward()
                      
                     if args.gradient_norm > 0.0:
@@ -153,8 +167,17 @@ def main(args):
                     scaler.step(optimizer)
                     scaler.update()
                 else:
-                    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
+                    log_probs, logits = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
                     _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)
+
+                    AFlogits = []
+                    for lgt, m in zip(logits, mask_for_loss):
+                        AFlogits.append(lgt[m>0][:,:-1])  # mask out unwanted part, and take only 20 amino acids
+
+                    # apply structure consistency loss using AFDistill
+                    sc_loss = afdistill.sc_loss(AFlogits).mean()
+                    loss_av_smoothed += 1.0 * torch.exp(-sc_loss) # modify weighting (1.0), if necessary
+
                     loss_av_smoothed.backward()
 
                     if args.gradient_norm > 0.0:
@@ -176,7 +199,7 @@ def main(args):
                 validation_acc = 0.
                 for _, batch in enumerate(loader_valid):
                     X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)
-                    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
+                    log_probs, _ = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)
                     mask_for_loss = mask*chain_M
                     loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)
                     
@@ -245,7 +268,7 @@ if __name__ == "__main__":
     argparser.add_argument("--rescut", type=float, default=3.5, help="PDB resolution cutoff")
     argparser.add_argument("--debug", type=bool, default=False, help="minimal data loading for debugging")
     argparser.add_argument("--gradient_norm", type=float, default=-1.0, help="clip gradient norm, set to negative to omit clipping")
-    argparser.add_argument("--mixed_precision", type=bool, default=True, help="train with mixed precision")
+    argparser.add_argument("--mixed_precision", type=bool, default=False, help="train with mixed precision")
  
     args = argparser.parse_args()    
     main(args)   
