diff --git a/run_cpd.py b/run_cpd.py
index f781a90..0aa712a 100644
--- a/run_cpd.py
+++ b/run_cpd.py
@@ -9,11 +9,11 @@ parser.add_argument('--max-nodes', metavar='N', type=int, default=3000,
                     help='max number of nodes per batch, default=3000')
 parser.add_argument('--epochs', metavar='N', type=int, default=100,
                     help='training epochs, default=100')
-parser.add_argument('--cath-data', metavar='PATH', default='./data/chain_set.jsonl',
-                    help='location of CATH dataset, default='./data/chain_set.jsonl')
-parser.add_argument('--cath-splits', metavar='PATH', default='./data/chain_set_splits.json',
+parser.add_argument('--cath-data', metavar='PATH', default='./data/chain_set.jsonl',
+                    help='location of CATH dataset, default='./data/chain_set.jsonl')
+parser.add_argument('--cath-splits', metavar='PATH', default='./data/chain_set_splits.json',
                     help='location of CATH split file, default='./data/chain_set_splits.json')
-parser.add_argument('--ts50', metavar='PATH', default='./data/ts50.json',
+parser.add_argument('--ts50', metavar='PATH', default='./data/ts50.json',
                     help='location of TS50 dataset, default='./data/ts50.json')
 parser.add_argument('--train', action="store_true", help="train a model")
 parser.add_argument('--test-r', metavar='PATH', default=None,
@@ -38,6 +38,12 @@ import torch_geometric
 from functools import partial
 print = partial(print, flush=True)
 
+import sys
+sys.path.insert(0, '</path/to/AFDistill/directory>')
+from model.litdistiller import LitDistiller
+
+afdistill = LitDistiller.load_from_checkpoint(checkpoint_path='/path/to/AFDistill/output/plddt_v1/checkpoints/checkpoint_last.ckpt', alphabet_downstream='LAGVESIKRDTPNQFYMHCW').cuda()
+
 node_dim = (100, 16)
 edge_dim = (32, 1)
 device = "cuda" if torch.cuda.is_available() else "cpu"
@@ -54,8 +60,8 @@ def main():
     model = gvp.models.CPDModel((6, 3), node_dim, (32, 1), edge_dim).to(device)
     
     print("Loading CATH dataset")
-    cath = gvp.data.CATHDataset(path="data/chain_set.jsonl",
-                                splits_path="data/chain_set_splits.json")    
+    cath = gvp.data.CATHDataset(path="./data/chain_set.jsonl",
+                                splits_path="./data/chain_set_splits.json")
     
     trainset, valset, testset = map(gvp.data.ProteinGraphDataset,
                                     (cath.train, cath.val, cath.test))
@@ -148,8 +154,16 @@ def loop(model, dataloader, optimizer=None):
         h_E = (batch.edge_s, batch.edge_v)
         
         logits = model(h_V, batch.edge_index, h_E, seq=batch.seq)
-        logits, seq = logits[batch.mask], batch.seq[batch.mask]
-        loss_value = loss_fn(logits, seq)
+        logits_masked, seq = logits[batch.mask], batch.seq[batch.mask]
+        loss_value = loss_fn(logits_masked, seq)
+
+        AFlogits = []
+        for i in torch.arange(batch.batch[-1] + 1):
+            AFlogits.append(logits[batch.batch == i])
+
+        # apply structure consistency loss using AFDistill
+        sc_loss = afdistill.sc_loss(AFlogits).mean()
+        loss_value += 1.0 * torch.exp(-sc_loss) # modify weighting (1.0), if necessary
 
         if optimizer:
             loss_value.backward()
@@ -158,7 +172,7 @@ def loop(model, dataloader, optimizer=None):
         num_nodes = int(batch.mask.sum())
         total_loss += float(loss_value) * num_nodes
         total_count += num_nodes
-        pred = torch.argmax(logits, dim=-1).detach().cpu().numpy()
+        pred = torch.argmax(logits_masked, dim=-1).detach().cpu().numpy()
         true = seq.detach().cpu().numpy()
         total_correct += (pred == true).sum()
         confusion += confusion_matrix(true, pred, labels=range(20))
@@ -183,4 +197,4 @@ def print_confusion(mat, lookup):
     print(res)
     
 if __name__== "__main__":
-    main()
\ No newline at end of file
+    main()
